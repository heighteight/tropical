% !TEX root = /Users/paolopistone/Documents/GitHub/tropicalnew/CSL24/main.tex

Before discussing how full-scale higher-order programming languages can be interpreted in terms of tropical power series, we highlight how such functions may naturally arise in the study of effectful programming languages.
We will see that, when considering probabilistic and non-deterministic programs, tropical tools can be used to describe the behavior of programs in \emph{the best/worst case}, and may lead to collapse the description of infinitely many possible behaviors into a combinatorial account of the optimal ones.
%Moreover, since, as we'll see, tropical semantics is also a \emph{metric} semantics, it can be used to study how much program behavior is \emph{sensitive} to errors, that is, how a small error in input may be increased in output. 




\subparagraph*{Maximum Likelihood Estimators for Probabilistic Languages}

%
%Tropical methods have been largely applied as a means to solve optimization problems. Typically, to solve a problem of the form 
%$$
%\mathrm{maximize} \ \ p(x_{1},\dots, x_{n})
%$$
%where $p(x_{1},\dots, x_{n})$ is some polynomial function, one can instead try to solve the (generally much simpler) problem of maximizing the associated tropical polynomial $\mathsf tp$ instead.



Let us start with a very basic probabilistic language:
the terms are $M::= \true \mid \false \mid M\oplus_p M$, for $p\in[0,1]$, and the operational semantics is $M\oplus_p N\to pM$ and $M\oplus_p N \to (1-p)N$, so that $M\oplus_p N$ plays the role of a probabilistic coin toss of bias $p$.
Consider the program
$
 M:=(\true \oplus_p\false)\oplus_p((\true\oplus_p\false)\oplus_p(\false\oplus_p\true)).
 $
 Calling $q=1-p$, to each occurrence of $\true$ or $\false$ in $M$, univocally determined by an address
$\omega\in \{l,r\}^{*}$, is associated a monomial $P_{\omega}(p,q)$ which determines the probability of the event ``$M\twoheadrightarrow_{\omega} \true/\false$'', that is, that $M$ reduces to $\true/\false$ according to the choices in $\omega$.
Thinking of $p,q$ as parameters, $P_{\omega}(p,q)$ can thus be read as the \emph{likelihood function} of the event ``$M\twoheadrightarrow_{\omega} \true/\false$''.
 For instance, we have
$P_{rll}(p,q):=qp^2$,
$P_{rrr}(p,q):=q^3$, and 
$P_{rrl}(p,q)=P_{rlr}(p,q):=q^2p$.
The polynomial function $Q_{\true}(p,q):=P_{ll}(p,q)+P_{rll}(p,q)+P_{rrr}(p,q)=p^2+p^2q+q^3$ gives instead the probability of the event ``$M\twoheadrightarrow \true$'', and analogously for $Q_{\false}(p,q):=P_{lr}(p,q)+P_{rrl}(p,q)+P_{rlr}(p,q)=pq+2pq^2$.

This way, the probabilistic evaluation of $M$ is presented as a \emph{hidden Markov model} \cite{Baum1966}, a fundamental statistical model, and notably one to which tropical methods are generally applied \cite{Pachter2004}.
Then, a natural question in this case, for a fixed $\omega_0$, is the following:
%
%The tropical point of view allows now to express two natural questions about this situation:
%\begin{enumerate}
% \item What is the \emph{maximum likelihood estimator} for the event ``$M\twoheadrightarrow_{\omega_0} \false$''  (similarly for $\true$)?
% I.e., which is the choice of $p,q$ that maximizes the probability $P_{\omega_0}$?
% \item 
knowing that $M$ reached a normal form, say $\true$, %produced $\false$ (similarly for $\true$),
 what is the \emph{maximum likelihood estimator} for the event ``$M\twoheadrightarrow_{\omega_0}\true$''? %, knowing that ``$M\twoheadrightarrow \false$''?
In other words, what is the choice of $p,q$ that maximizes the conditional probability $\BB P(M\twoheadrightarrow_{\omega_0} \true\mid M\twoheadrightarrow \true)$, i.e.~that 
 makes $\omega_0$ the most likely path among those leading to $\true$?
% \mid ``M\twoheadrightarrow \false'')$)?
%\end{enumerate}
%
%Answering 1) amounts at solving a constrained maximization problem:
%\[(p,q)\in\arg\max_{y=1-x} P_{\omega_{0}}(x,y)\] or, equivalently: $p\in\arg\max_{x\in[0,1]} P_{\omega_0}(x,1-x)$,
%related to the monomial $P_{\omega_{0}}(p,q)$ and for which a tropical transformation does not, in general, help factoring the contraint $q=1-p$ out of the maximization.
%We will, instead, mainly concentrate on 2).

Answering this question amounts at finding a solution to the following constrained maximization problem in the unknown $p\in[0,1]$:
%\[
%P_{\omega_0}(p,q) = \max_\omega P_{\omega}(p,q)
%
%
%\]
%
%\[
%(p,q) \in \arg\max_{y=1-x} P_{\omega_{0}}(x,y)\] or, equivalently: $p\in\arg\max_{x\in[0,1]} P_{\omega_0}(x,1-x)$,
%
%\]
% in the unknown $p\in[0,1]$:
\[
P_{\omega_0}(p,1-p) = \max_\omega P_{\omega}(p,1-p)
\]
which is related to the polynomial, say, $Q_{\true}(p,q)$.
% the difficulty raised by the constraint between the two variables of the monomials might be overcome by passing to the associated \emph{tropical} polynomial.
%In fact, 
Since $-\log P_\omega(p,q)=(\trop P_{\omega})(-\log p,-\log q)$, this is equivalent
% to the following \emph{minimization} problem in the unknown $p\in[0,1]$:
%\[
%(\trop P_{\omega_0})(-\log p,-\log(1-p)) = (\trop Q_{\true})(-\log p,-\log(1-p))
%\]
%%With a change of variable we can now move the  constraint outside the minimization problem, as
%which is in turn equivalent
 to finding a solution of the following constrained \emph{minimization} problem in the unknowns $(p,x,y)\in[0,1]\times[0,\infty]\times[0,\infty]$:
\begin{equation}\label{eq:troproot}
\trop P_{\omega_{0}} (x,y)= \trop Q_{\true}(x,y) , \qquad x= -\log p,\qquad y=-\log(1-p).
\tag{$\star$}
\end{equation}
Since the first equation can be solved easily (i.e.~in linear time) by computing the tropical roots of $\trop Q_{\true}$, we can obtain an explicit relation between $x$ and $y$ that can be used to solve the whole system, finally finding our $p$, as the next example shows.
%Problem 1) the maximum values $x,y$ of $P_{rll}(p,q)$ can be computed by finding the \emph{minimum} values of $\mathsf tP_{rll}(-\log p, -\log q)= -2\log p- \log q$ under the contraint $q=1-p$. Notice that the latter is precisely the \emph{negative log-probability} of the event ``$M\twoheadrightarrow_{rll} \false$''.
\begin{example}
For our running example $M$, 
let us suppose that we observed the event ``$M\twoheadrightarrow\true$'', so that our probabilities are conditioned under this observation. We have
\[\trop Q_{\true}(x,y)=\min\set{\trop P_{ll}(x,y),\trop  P_{rll}(x,y),\trop P_{rrr}(x,y)}=\min
\set{2x,y+2x,3y}.\]
The tropical roots of $\trop Q_{\true}(x,y)$ are all the points of the form $(x,\frac{2}{3}x)$. Recall that these are the points where \eqref{eq:troproot} is satisfied for \emph{at least two} distinct values of $\omega_{0}$ (indeed for $\omega_{0}\in \{ll, rrr\}$). From this it follows that $\trop Q_{\true} (x,y)=\trop P_{rrr}(x,y)=3y$ holds iff $y\leq \frac{2}{3}x$, and  $\trop Q_{\true}(x,y)=2x=\trop P_{ll}(x,y)$ otherwise.
In this way can find the maximum likelihood estimator for $\omega_0=rrr$:
via the substitution $x:=-\log p$, $y:=-\log (1-p)$, the condition $y\leq \frac{2}{3}x$ is equivalent to $-\log (1-p)\leq -\frac{2}{3}\log p$, i.e.\ $1-p\geq p^{\frac{2}{3}}$.
This means that, if $p\in[0,1]$ satisfies $1-p\geq p^{\frac{2}{3}}$ (for example, $p=\frac{1}{4}$), then $P_{rrr}(p,1-p) = \max_{\omega=ll, rll, rrr} P_{\omega}(p,1-p)$. In other words, knowing that $M$ sampled $\true$ in its normal form, the most likely sampled occurrence of $\true$ is the one at the address $rrr$ iff $1-p\geq p^{\frac{2}{3}}$.
%Said differently, the maximum likelihood estimator of $p$, with prior knowledge  ``$M\twoheadrightarrow \true$'' and seen the event ``$M\twoheadrightarrow_{\omega_0}\true$'', is any $p$ s.t.\ $1-p\geq p^{\frac{2}{3}}$. 
%Observe that the tropical roots of $\trop Q_{\true}(x,y)$ provide a complete description of the relationship between events and their maximum likelihood estimators. 
%We have thus solved problem 2) above for $\omega_0=rrr$, under the additional hypothesis that ``$M\twoheadrightarrow\true$''.
%Remembering that $2x=\trop P_{ll}(x,y)$, in this simple case we can even say that for the other values of $p$ (for example, $p=\frac{1}{2}$), the most likely $\true$ to be sampled (knowing that a $\true$ has been sampled) is the one at address $ll$, therefore actually solving problem 2), under the same additional hypothesis, for all instances of the reduction path $\omega$ leading to $\true$ (the path $rll$ being the most likely one for no $p$ at all).
\end{example}
%the maximum values of $Q_{\true}(p,q): [0,1]^{2}\to [0,1]$ can be computed as $e^{-\alpha},e^{-\beta}$, where $\alpha,\beta\in[0,\infty]$ are the \emph{minimum} values of the tropical polynomial $\mathsf t Q_{\true}(\alpha,\beta) = \min \{ 2\alpha, 2\alpha+\beta, 3\beta\}$.

As we'll see in Section \ref{section3}, this analysis extends to PCF-style programs. For example, the program $M=\mathbf Y(\lambda x.\true \oplus_{p} x)$ yields the power series $Q_{\true}(p,q)=\sum_{n=0}^{\infty}pq^{n}=\frac{p}{1-q}$ that sums all \emph{infinitely many} ways in which $M$ may reduce to $\true$. Notice that the tropicalised series $\mathsf tQ_{\true}(-\log p,-\log q)=\inf_{n\in \mathbb N}\{-\log p -n\log q\}=-\log p$ collapses onto a single monomial describing the \emph{unique} most likely reduction path of $M$ leading to $\true$, namely the one that passes through a coin toss only once. 
%
%Similar arguments could be done by replacing $\true$ and $\false$ by, respectively, a converging and diverging term (e.g.~in a $\mathrm{PCF}$-style language), so 1) would be about finding maximum likelihood estimators for the event ``$M$ converges''.


\subparagraph*{Best Case Analysis for Non-Deterministic Languages}

This example is inspired from \cite{Manzo2013}. We consider now a basic non-deterministic language with terms $M::= \true \mid  \mathtt{Gen} \mid  M + M$, with an operation semantics comprising a non-deterministic reduction rule 
$M_{1}+M_{2} \stackrel{\alpha}{\to} M_{i}$ and a generation rule
$\mathtt{Gen}\stackrel{\beta}{\to} \true +\mathtt{Gen}$, 
where in each case the value $\alpha,\beta\in \Lawv$ indicates a \emph{cost} associated with the reduction (e.g.~the estimated clock value for the simulation of each reduction on a given machine model). 
Then, any reduction $\omega: M \twoheadrightarrow N$ of a term to (one of its) normal form is  associated with a tropical monomial $P_{\omega}( \alpha,\beta)$ consisting of the sum of the costs of all reductions in $\omega$. For a given normal form $N$, the reductions $\omega_{i}: M \twoheadrightarrow N$ give rise to a tps $\inf_{i\in I}P_{\omega_{i}}( \alpha,\beta)$. 
For example, consider the non-deterministic term
$
M :=\mathtt{Gen}+  ((\true + \true) + \mathtt{Gen})
$. 
The (infinitely many) reduction paths leading to $\true$ can be grouped as follows:
\begin{itemize}
\item left, then reduce $\mathtt{Gen}$ $n+1$-times, then left;
\item right, then left and then either left or right;
\item right twice, then reduce $\mathtt{Gen}$ $n+1$-times and then left.
\end{itemize}
This leads to the tps 
$\varphi_{M\twoheadrightarrow \true}(\alpha)=\inf_{n\in \mathbb N}\big\{(n+2)\alpha+(n+1)\beta, 3\alpha, (n+3)\alpha+(n+1)\beta\big \}= \min\{ 2\alpha+\beta, 3\alpha\}$, which describes all possible behaviors of $M$. Notice that, since $\alpha$ and $\beta$ are always positive, the power series $\varphi_{M\twoheadrightarrow\true}(\alpha)$ is indeed equivalent to the tropical polynomial $\min\{ 2\alpha+\beta, 3\alpha\}$. In other words, of the infinitely many behaviors of $M$, only finitely many have chances to be \emph{optimal}: either left + $\mathtt{Gen}$ + left, or right + left + (left,right). Also in this case, reducing to best-case analysis leads to collapse the infinitary description of \emph{all} behaviors to a purely combinatorial description of the finitely many optimal ones. 

Once reduced $\varphi_{M\twoheadrightarrow\true}$ to a polynomial, the best behavior among these will depend on the values of $\alpha$ and $\beta$, and by studying the tropical polynomial $\varphi_{M\twoheadrightarrow\true}$ one can thus answer questions analogous to 2) above, that is, what are the best choices of costs $\alpha,\beta$ making a \emph{chosen} reduction of $M$ to $\true$ the cheapest one?



















